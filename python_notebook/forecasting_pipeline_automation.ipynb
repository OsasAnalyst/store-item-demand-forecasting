{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook automates the entire forecasting workflow - from loading data and preprocessing it, to generating predictions and saving results.\n",
        "Automation is important because it keeps the process consistent and repeatable, especially when dealing with time-series updates or new data over time. Instead of manually running each step or copying code across notebooks, the pipeline handles everything end-to-end with a single function call.\n",
        "\n",
        "The results are then saved both as CSV files and into a local SQLite database for easy access and future analysis.\n",
        "\n",
        "By the end, this setup makes it simple to update forecasts whenever new data arrives, no code rewriting, no manual data handling, just run the pipeline and get updated predictions.\n",
        "\n",
        "Codes here are from the main `demand_forecasting_ecommerce.ipynb` notebook, it was wrapped in a function to automate the process."
      ],
      "metadata": {
        "id": "hs5h5PH6i-87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sqlalchemy import text"
      ],
      "metadata": {
        "id": "FsunY75SDU-M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_data(df, scaler=None, fit_scaler=False):\n",
        "    \"\"\"\n",
        "      - Convert date to datetime\n",
        "      - Sort by store,item,date\n",
        "      - Create time features\n",
        "      - If `sales` exists: create log_sales, sales_diff (groupwise)\n",
        "      - Scale `sales` using StandardScaler (fit only if fit_scaler=True)\n",
        "\n",
        "    Returns:\n",
        "      df (pd.DataFrame) : transformed dataframe with new columns\n",
        "      scaler (StandardScaler or None)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Ensure datetime\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "    # Sort for group operations\n",
        "    df = df.sort_values([\"store\", \"item\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "    # Add time features\n",
        "    df[\"year\"] = df[\"date\"].dt.year\n",
        "    df[\"month\"] = df[\"date\"].dt.month\n",
        "    df[\"day\"] = df[\"date\"].dt.day\n",
        "    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
        "    df[\"weekofyear\"] = df[\"date\"].dt.isocalendar().week.astype(int)\n",
        "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
        "\n",
        "    # If sales present, create log and diff features\n",
        "    if \"sales\" in df.columns:\n",
        "        # log transform\n",
        "        df[\"log_sales\"] = np.log1p(df[\"sales\"])\n",
        "\n",
        "        # Group-wise differencing\n",
        "        df[\"sales_diff\"] = df.groupby([\"store\", \"item\"])[\"log_sales\"].diff()\n",
        "\n",
        "        # Scale raw sales for ML models\n",
        "        if scaler is None:\n",
        "            scaler = StandardScaler()\n",
        "        if fit_scaler:\n",
        "            df[\"sales_scaled\"] = scaler.fit_transform(df[[\"sales\"]])\n",
        "        else:\n",
        "            df[\"sales_scaled\"] = scaler.transform(df[[\"sales\"]])\n",
        "    else:\n",
        "        scaler = scaler\n",
        "\n",
        "    return df, scaler"
      ],
      "metadata": {
        "id": "Vi6pX4dGGpAP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lag and rolling feature\n",
        "def create_lag_rolling_features(df, lags=[1,7,14,28], roll_windows=[7,14,28], target_col=\"lag_sales\"):\n",
        "    \"\"\"\n",
        "    Add lag_n and rolling_mean_w features based on `target_col`.\n",
        "    Uses groupby by (store, item) and shift/rolling so no leakage occurs.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_values([\"store\", \"item\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "    # Lags\n",
        "    for lag in lags:\n",
        "        df[f\"lag_{lag}\"] = df.groupby([\"store\", \"item\"])[target_col].shift(lag)\n",
        "\n",
        "    # Rolling means\n",
        "    for w in roll_windows:\n",
        "        df[f\"rolling_mean_{w}\"] = df.groupby([\"store\", \"item\"])[target_col] \\\n",
        "                                      .transform(lambda x: x.shift(1).rolling(window=w, min_periods=1).mean())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "_oqKyYqfHGmk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X, y builder\n",
        "def build_X_y(df, feature_cols, target_col=\"sales\"):\n",
        "    \"\"\"\n",
        "    Given a dataframe with features (including lag features), return X, y.\n",
        "    Drops rows with NaNs in any of the chosen feature_cols or the target (to avoid training on incomplete rows).\n",
        "    For test (no target_col), returns X and y=None.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    if target_col in df.columns:\n",
        "        # drop rows where any feature or target is NaN\n",
        "        keep = df.dropna(subset=feature_cols + [target_col])\n",
        "        X = keep[feature_cols].reset_index(drop=True)\n",
        "        y = keep[target_col].reset_index(drop=True)\n",
        "        return X, y\n",
        "    else:\n",
        "        # Test data: drop rows any feature is Nan, return X only\n",
        "        keep = df.dropna(how=\"all\", subset=feature_cols)\n",
        "        X = keep[feature_cols].reset_index(drop=True)\n",
        "        return X, None"
      ],
      "metadata": {
        "id": "kRJY2ENgHIZq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directories for saving outputs\n",
        "def setup_directories():\n",
        "    \"\"\"Create folders for outputs and models.\"\"\"\n",
        "    os.makedirs(\"model_outputs\", exist_ok=True)\n",
        "    os.makedirs(\"database\", exist_ok=True)\n",
        "    return \"database/forecasting.db\""
      ],
      "metadata": {
        "id": "-CydJtA_HNKp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets (train, validation, and test)\n",
        "def load_datasets():\n",
        "    \"\"\"Load training, validation, and test data.\"\"\"\n",
        "    print(\"Loading datasets...\")\n",
        "    train_data = pd.read_csv(\"train_data.csv\")\n",
        "    val_data = pd.read_csv(\"val_data.csv\")\n",
        "    test = pd.read_csv(\"test.csv\")\n",
        "    print(f\"Train: {train_data.shape} | Val: {val_data.shape} | Test: {test.shape}\")\n",
        "    return train_data, val_data, test"
      ],
      "metadata": {
        "id": "8fU7nKFrHP9M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess train/val/test datasets with the same preprocessing function\n",
        "def preprocess_all(train_data, val_data, test):\n",
        "    \"\"\"Preprocess all datasets and apply feature engineering.\"\"\"\n",
        "    print(\"Preprocessing data...\")\n",
        "    train_proc, scaler = preprocess_data(train_data, fit_scaler=True)\n",
        "    val_proc, _ = preprocess_data(val_data, scaler=scaler, fit_scaler=False)\n",
        "    test_proc, _ = preprocess_data(test, scaler=scaler, fit_scaler=False)\n",
        "\n",
        "    # Mark dataset origin\n",
        "    train_proc[\"__set__\"] = \"train\"\n",
        "    val_proc[\"__set__\"] = \"val\"\n",
        "    test_proc[\"__set__\"] = \"test\"\n",
        "\n",
        "    # Combine for feature creation\n",
        "    combined = pd.concat([train_proc, val_proc, test_proc], axis=0, ignore_index=True)\n",
        "    combined = combined.sort_values([\"store\", \"item\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "    # Create lag and rolling features\n",
        "    combined = create_lag_rolling_features(combined, target_col=\"log_sales\")\n",
        "\n",
        "    # Split back into train, validation, and test\n",
        "    train_lag = combined[combined[\"__set__\"] == \"train\"].reset_index(drop=True)\n",
        "    val_lag   = combined[combined[\"__set__\"] == \"val\"].reset_index(drop=True)\n",
        "    test_lag  = combined[combined[\"__set__\"] == \"test\"].reset_index(drop=True)\n",
        "\n",
        "    # Define feature columns (excluding target 'sales' and 'log_sales')\n",
        "    features = [\n",
        "        \"store\", \"item\", \"year\", \"month\",\n",
        "        \"dayofweek\", \"is_weekend\", \"weekofyear\",\n",
        "        \"lag_1\", \"lag_7\", \"lag_14\", \"lag_28\",\n",
        "        \"rolling_mean_7\", \"rolling_mean_14\", \"rolling_mean_28\"\n",
        "    ]\n",
        "\n",
        "    # Build X, y for training, validation, and test sets\n",
        "    X_train, y_train = build_X_y(train_lag, feature_cols=features, target_col=\"sales\")\n",
        "    X_val, y_val = build_X_y(val_lag, feature_cols=features, target_col=\"sales\")\n",
        "    X_test, _ = build_X_y(test_lag, feature_cols=features, target_col=None)\n",
        "\n",
        "    # Fill missing values in test data\n",
        "    X_test = X_test.fillna(X_train.mean())\n",
        "\n",
        "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "    print(f\"X_test: {X_test.shape}\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, test, train_data, val_data"
      ],
      "metadata": {
        "id": "BvzNOgA7HTL8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained models\n",
        "def load_models():\n",
        "    \"\"\"Load pre-trained models.\"\"\"\n",
        "    print(\"Loading trained models...\")\n",
        "    xgb = joblib.load(\"xgb_model.pkl\")\n",
        "    lgbm = joblib.load(\"lgbm_model.pkl\")\n",
        "    rf = joblib.load(\"rf_model.pkl\")\n",
        "    return xgb, lgbm, rf"
      ],
      "metadata": {
        "id": "peIdJPZWHYxa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "def generate_predictions(xgb, lgbm, rf, X_test, test):\n",
        "    \"\"\"Generate and combine forecasts from models.\"\"\"\n",
        "    print(\"Generating forecasts...\")\n",
        "    xgb_preds = xgb.predict(X_test)\n",
        "    lgbm_preds = lgbm.predict(X_test)\n",
        "    rf_preds = rf.predict(X_test)\n",
        "    ensemble_preds = (0.5 * xgb_preds) + (0.5 * lgbm_preds)\n",
        "\n",
        "    test_forecast = pd.concat(\n",
        "        [test.reset_index(drop=True),\n",
        "         pd.DataFrame({\n",
        "             \"XGB_Preds\": xgb_preds,\n",
        "             \"LGBM_Preds\": lgbm_preds,\n",
        "             \"RF_Preds\": rf_preds,\n",
        "             \"Ensemble\": ensemble_preds\n",
        "         })],\n",
        "        axis=1\n",
        "    )\n",
        "    return test_forecast"
      ],
      "metadata": {
        "id": "PdVPKe4lHcQ1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV and database\n",
        "def save_results(test_forecast, train_data, val_data, db_path):\n",
        "    \"\"\"Save the predictions and original data to CSV and SQLite.\"\"\"\n",
        "    test_forecast.to_csv(\"model_outputs/test_forecast_predictions.csv\", index=False)\n",
        "    print(\"Predictions saved to 'model_outputs/test_forecast_predictions.csv'\")\n",
        "\n",
        "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
        "    train_data.to_sql(\"train_data\", engine, index=False, if_exists=\"replace\")\n",
        "    val_data.to_sql(\"val_data\", engine, index=False, if_exists=\"replace\")\n",
        "    test_forecast.to_sql(\"test_predictions\", engine, index=False, if_exists=\"replace\")\n",
        "    print(f\"Data stored successfully in {db_path}\")"
      ],
      "metadata": {
        "id": "NW4_olMBAjwz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_forecasting_pipeline():\n",
        "    \"\"\"Run the entire forecasting workflow from start to finish.\"\"\"\n",
        "    db_path = setup_directories()\n",
        "    train_data, val_data, test = load_datasets()\n",
        "    X_train, y_train, X_val, y_val, X_test, test_df, train_raw, val_raw = preprocess_all(train_data, val_data, test)\n",
        "    xgb, lgbm, rf = load_models()\n",
        "    test_forecast = generate_predictions(xgb, lgbm, rf, X_test, test_df)\n",
        "    save_results(test_forecast, train_raw, val_raw, db_path)\n",
        "    print(\"Forecasting pipeline completed successfully.\")"
      ],
      "metadata": {
        "id": "W4_bOQ-EHlAm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the entire pipeline\n",
        "run_forecasting_pipeline()"
      ],
      "metadata": {
        "id": "lfRs_I5XHrBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e2949a-d9a2-4ad1-bde7-66dae7ff1a9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Train: (730500, 4) | Val: (182500, 4) | Test: (45000, 4)\n",
            "Preprocessing data...\n",
            "X_train: (716500, 14), y_train: (716500,)\n",
            "X_val: (182500, 14), y_val: (182500,)\n",
            "X_test: (45000, 14)\n",
            "Loading trained models...\n",
            "Generating forecasts...\n",
            "Predictions saved to 'model_outputs/test_forecast_predictions.csv'\n",
            "Data stored successfully in database/forecasting.db\n",
            "Forecasting pipeline completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLEAN DATA TYPES FOR POWER BI**"
      ],
      "metadata": {
        "id": "9kfX7PVkR4gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create power bi friendly table\n",
        "db_path = \"database/forecasting.db\"\n",
        "engine = create_engine(f\"sqlite:///{db_path}\")\n",
        "with engine.connect() as conn:\n",
        "    conn.execute(text(\"DROP VIEW IF EXISTS forecast_powerbi_view;\"))\n",
        "    conn.execute(text(\"DROP TABLE IF EXISTS forecast_clean;\"))\n",
        "\n",
        "    conn.execute(text(\"\"\"\n",
        "          CREATE TABLE forecast_clean AS\n",
        "        SELECT\n",
        "            CAST(id AS INTEGER) AS id,\n",
        "            CAST(date AS TEXT) AS date,\n",
        "            CAST(store AS INTEGER) AS store,\n",
        "            CAST(item AS INTEGER) AS item,\n",
        "            CAST(XGB_Preds AS FLOAT) AS XGB_Preds,\n",
        "            CAST(LGBM_Preds AS FLOAT) AS LGBM_Preds,\n",
        "            CAST(RF_Preds AS FLOAT) AS RF_Preds,\n",
        "            CAST(Ensemble AS FLOAT) AS Ensemble\n",
        "        FROM test_predictions;\n",
        "    \"\"\"))\n",
        "\n",
        "print(\"forecast_clean table created successfully for Power BI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GkSvxOkRvJb",
        "outputId": "b4281eeb-41a6-4e35-a598-7be866086ee0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forecast_clean table created successfully for Power BI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TudyyN1b7Kla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}